### 오늘 한 공부

오늘은 딥러닝 쪽에서 **경사하강법 최적화 방법이랑 CNN 데이터 처리 흐름** 위주로 정리했다.

---

### 경사하강법 최적화

* **SGD**: 전체 데이터 대신 랜덤 샘플로 학습해서 속도 높임.
* **Momentum**: 이전 방향을 참고해서 관성처럼 밀고 나감. Local Minima에 덜 갇힘.
* **AdaGrad**: 변수마다 학습률을 다르게 조정. 자주 업데이트되는 파라미터는 느리게, 덜 바뀌는 건 빠르게.
* **Adam**: 모멘텀 + AdaGrad 합친 버전. 요즘 거의 기본값으로 많이 씀.

---

### 가중치 초기화

* **Xavier 초기화**: Sigmoid, Tanh에서 사용.
* **He 초기화**: ReLU에서 사용.
* 초기값이 잘못되면 기울기 소실로 학습이 멈출 수 있음.

---

### 과적합 방지

* **Dropout**: 학습 시 뉴런 일부를 랜덤으로 꺼서 특정 노드에 의존하지 않게 함.
* 테스트할 땐 다시 전체 뉴런 사용.

---

### CNN 데이터 파이프라인 (Keras)

* 예전엔 `ImageDataGenerator()`를 썼지만, 요즘은
  `image_dataset_from_directory()`로 바로 불러오는 게 일반적.
* 이 방식은 `tf.data` 기반이라 빠르고, 폴더 이름을 자동으로 라벨로 인식함.

**전처리 / 증강**

* `Rescaling(1/255)`은 모든 데이터셋에 적용 (train/val/test 모두).
* `RandomFlip`, `RandomRotation` 같은 증강은 train에서만 적용됨.
* 증강은 미리 복사하는 게 아니라, 매 epoch마다 랜덤하게 실시간 변형됨.

---

### 예측과 평가

* `glob.glob()`은 이미지가 아니라 파일 이름 목록을 가져오는 함수.
* 예측할 때는 `image.load_img()` → `img_to_array()` → `np.expand_dims()` 순서로 처리해야 모델 입력이 됨.
* 정확도는 `model.evaluate(test_ds)`로 자동 계산 가능.
* `loss='mse'`, `metrics=['rmse']` 조합은 학습과 해석의 단위 차이를 맞추기 위해 씀.

---

### 느낀 점

Adam이 기본 옵티마이저로 쓰이는 이유를 직접 이해하게 됐고,
데이터 증강이 단순 복제가 아니라 실시간 변형이라는 게 새로웠다.
딥러닝 학습 과정이 생각보다 '데이터 처리 구조'에 크게 의존한다는 점도 인상 깊었음.
